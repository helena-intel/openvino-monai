{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871269e-d4de-47fc-9485-ef91f3daa66f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training, Optimization and Quantization with MONAI, PyTorch Lightning and OpenVINO\n",
    "\n",
    "This tutorial shows how to train a [MONAI](https://monai.io/) classification model on a [MedMNIST](https://medmnist.com/) dataset, and quantize the model with [OpenVINO's Post-Training Optimization Tool](https://docs.openvino.ai/latest/pot_README.html)\n",
    "\n",
    "To run this notebook, please create a virtual environment with Python 3.7 or 3.8, with `python -m venv monai_env` (on Linux use `python3`) and install the requirements with `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2dfd8-e5fa-4439-9862-3b48dc4082eb",
   "metadata": {},
   "source": [
    "## Supported Networks and Datasets\n",
    "\n",
    "The following MONAI Classification Networks are supported in this notebook:\n",
    "\n",
    "`[\"DenseNet\",\"SENet154\", \"SEResNet50\",  \"SEResNext50\"]`. \n",
    "\n",
    "The variants of these networks `\"DenseNet121\", \"DenseNet169\", \"DenseNet201\", \"DenseNet264\",  \"SEResNet101\", \"SEResNet152\", \"SEResNext101\"` also work. \n",
    "\n",
    "All MedMNIST datasets for multi-class and binary-class classification are supported: `['pathmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist', 'organmnist3d', 'nodulemnist3d', 'adrenalmnist3d', 'fracturemnist3d', 'vesselmnist3d', 'synapsemnist3d']`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0ff1e-08fa-4e3f-a4d9-7f0d65df2b91",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1c72-8d54-4192-8ffa-ee2ebab67c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import dateutil\n",
    "import matplotlib.pyplot as plt\n",
    "import medmnist\n",
    "import monai\n",
    "import monai.networks.nets as nets\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import nncf  # must be imported after torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from IPython.display import Markdown, display\n",
    "from monai.metrics import ConfusionMatrixMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AddChannel,\n",
    "    AsChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    ToTensor,\n",
    ")\n",
    "from openvino.inference_engine import IECore\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "from torch.jit import TracerWarning\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e314f-3b41-48ca-877d-ee2e21bcbbfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset, Metric and Model\n",
    "\n",
    "We start by defining a Dataset and DataModule to handle data transformation and loading, a Metric class that specifies how to evaluate the model, and the PyTorch Lightning Model that specifies the MONAI model to use and contains the training and evaluation code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4a936-87fa-475c-af5e-dd1042de4751",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "We create a Monai Dataset to load and transform the data, and a PyTorch Lighnting DataModule for accessing this data during training. The dataset returns data as a tuple consisting of (image, mask, image_metadata, mask_metadata).\n",
    "\n",
    "We use Monai Transforms to transform and augment the data during training. During training, we randomly rotate the data, add noise, and shift pixel values. Monai's ImageDataset ensures that the random seed for the image and segmentation mask transform are the same, and therefore that for the random rotation transforms, image and mask will be rotated in the same way. During validation, we only make sure that the dimensions and data type are correct.\n",
    "\n",
    "The specified MedMNIST dataset is downloaded if it has not been downloaded before. See the top of this notebook for the supported datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1586697-6d7a-4f45-ad8e-868c04d1f396",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fead4-d2d4-49e2-9dff-fd9f42c9bff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedMNISTDataset(Dataset):\n",
    "    def __init__(self, medmnist_dataset, split) -> None:\n",
    "\n",
    "        supported_datasets = [\n",
    "            (ds_name)\n",
    "            for (ds_name, ds_info) in medmnist.INFO.items()\n",
    "            if ds_info[\"task\"] in [\"multi-class\", \"binary-class\"]\n",
    "        ]\n",
    "\n",
    "        if medmnist_dataset not in supported_datasets:\n",
    "            raise ValueError(\n",
    "                f\"{medmnist_dataset} is not a supported dataset. Supported datasets are: \"\n",
    "                f\"{supported_datasets}.\"\n",
    "            )\n",
    "\n",
    "        dataset = getattr(\n",
    "            medmnist, medmnist.dataset.INFO[medmnist_dataset][\"python_class\"]\n",
    "        )\n",
    "        self.num_dims = 3 if medmnist_dataset.endswith(\"3d\") else 2\n",
    "        self.num_channels = medmnist.dataset.INFO[medmnist_dataset][\"n_channels\"]\n",
    "        self.labels = medmnist.dataset.INFO[medmnist_dataset][\"label\"]\n",
    "        self.num_classes = len(self.labels)\n",
    "\n",
    "        transforms = [ToTensor(dtype=torch.float)]\n",
    "        # transforms = []\n",
    "        if self.num_channels == 3:\n",
    "            transforms.append(AsChannelFirst())\n",
    "        elif self.num_dims == 2:\n",
    "            transforms.append(AddChannel())\n",
    "\n",
    "        transform = Compose(transforms)\n",
    "\n",
    "        print(\n",
    "            f\"Setup {medmnist_dataset} {split}, {self.num_channels} channels, \"\n",
    "            f\"{self.num_classes} classes\"\n",
    "        )\n",
    "        split_dataset = dataset(split=split, transform=None, download=True)\n",
    "\n",
    "        data = [\n",
    "            (transform(np.asarray(item[0])), torch.as_tensor(item[1]))\n",
    "            for item in split_dataset\n",
    "        ]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of elements in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get item from self.dataset at the specified index.\n",
    "        Returns (annotation, image), where annotation is a tuple (index, class_index)\n",
    "        and image a preprocessed image in network shape\n",
    "        \"\"\"\n",
    "        image, label = self.data[index]\n",
    "        image = torch.as_tensor(image, dtype=torch.float)\n",
    "        label = torch.as_tensor(label, dtype=torch.float)\n",
    "        # image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c94bc-d36d-4771-bc5b-eb1cbea6c33b",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning DataModule\n",
    "\n",
    "A [Lightning DataModule]( https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html) defines dataloaders and connects the Lightning Module to the Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e9fa6-3ec8-47b4-9c52-b36361e02753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, medmnist_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.medmnist_dataset = medmnist_dataset\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        random.seed(1.414213)\n",
    "\n",
    "        self.dataset_train = MedMNISTDataset(self.medmnist_dataset, \"train\")\n",
    "        self.dataset_val = MedMNISTDataset(self.medmnist_dataset, \"val\")\n",
    "        self.dataset_test = MedMNISTDataset(self.medmnist_dataset, \"test\")\n",
    "\n",
    "        print(f\"Setup train dataset: {len(self.dataset_train)} items\")\n",
    "        print(f\"Setup val dataset: {len(self.dataset_val)} items\")\n",
    "        print(f\"Setup test dataset: {len(self.dataset_test)} items\")\n",
    "\n",
    "        assert len(self.dataset_train) > 0, \"Train dataset is empty.\"\n",
    "        assert len(self.dataset_val) > 0, \"Val dataset is empty\"\n",
    "        assert len(self.dataset_test) > 0, \"Val dataset is empty\"\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59d94f-8e66-450c-8b43-d51daa68c098",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model\n",
    "\n",
    "We create a PyTorch Lightning [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) to train a [Monai network](https://docs.monai.io/en/latest/networks.html#nets)\n",
    "\n",
    "For binary classification models we use Binary Cross Entropy Loss, for multiclass segmentation Cross Entropy Loss. For optimizer, we use the Adam Optimizer with the default learning rate of 0.001. The evaluation metric is accuracy, implemented with the Accuracy class defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247110f4-9b80-4fdb-bccc-2448e4d0e833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MonaiModel(pl.LightningModule):\n",
    "    def __init__(self, monai_model: str, config: Dict):\n",
    "        \"\"\"\n",
    "        PyTorch Lightning Module for a given MONAI classification model.\n",
    "\n",
    "        :param monai_model: MONAI model name. For example, DenseNet, SEResNet50\n",
    "        :param config: Dictionary with configuration values to pass to MONAI model initialization.\n",
    "                       Dictionary keys that are not supported model parameters are discarded.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        model = getattr(nets, monai_model)\n",
    "        model_config = config.copy()\n",
    "        self.num_classes = model_config[\"num_classes\"]\n",
    "        \n",
    "        # For binary classification (2 classes, e.g. normal/abnormal) the model is configured\n",
    "        # with 1 class that can either be 0 or 1.\n",
    "        if model_config[\"num_classes\"] == 2:\n",
    "            model_config[\"num_classes\"] = 1\n",
    "\n",
    "        for parameter in model_config.copy():\n",
    "            if (\n",
    "                parameter not in inspect.signature(model).parameters\n",
    "                and parameter not in inspect.signature(model.__bases__[0]).parameters\n",
    "            ):\n",
    "                model_config.pop(parameter)\n",
    "        self._model = model(**model_config).cpu()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        # https://docs.monai.io/en/latest/highlights.html?deterministic-training-for-reproducibility\n",
    "        monai.utils.set_determinism(seed=2.71828, additional_settings=None)\n",
    "        self.loss_function = (\n",
    "            torch.nn.CrossEntropyLoss()\n",
    "            if self.num_classes > 2\n",
    "            else torch.nn.BCEWithLogitsLoss()\n",
    "        )\n",
    "        self.metric = ConfusionMatrixMetric(metric_name=\"accuracy\")\n",
    "        self.best_val_accuracy = 0\n",
    "        self.best_val_epoch = 0\n",
    "\n",
    "        print(\n",
    "            f\"Initialized {monai_model} with settings: {model_config} {self.num_classes} classes\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def forward_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Propagate images through the network\n",
    "\n",
    "        :return: raw network output in layout expected by loss function\n",
    "        \"\"\"\n",
    "        images, _ = batch\n",
    "        images = torch.as_tensor(images, dtype=torch.float)\n",
    "        output = self.forward(images)\n",
    "        if isinstance(self.loss_function, torch.nn.BCEWithLogitsLoss):\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.unsqueeze(-1)\n",
    "        return output\n",
    "\n",
    "    def process_labels(self, batch):\n",
    "        \"\"\"\n",
    "        Return labels in format expected by the loss function (expected to be\n",
    "        BCEWithLogitsLoss for binary classification and CrossEntropyLoss for multiclass\n",
    "\n",
    "        :return: labels in correct datatype and layout\n",
    "        \"\"\"\n",
    "        _, annotation = batch\n",
    "        labels = torch.as_tensor(annotation, dtype=torch.float)\n",
    "        if isinstance(self.loss_function, torch.nn.BCEWithLogitsLoss):\n",
    "            labels = labels.float()\n",
    "        else:\n",
    "            labels = labels.long().squeeze(dim=1)\n",
    "        return labels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_one(self, x):\n",
    "        \"\"\"\n",
    "        Propage one image through the network and return the result as a class index\n",
    "        Uses sigmoid for binary classification and argmax for multiclass\n",
    "\n",
    "        :return prediction as class index integer\n",
    "        \"\"\"\n",
    "        output = self.forward(x)\n",
    "        if self.num_classes <= 2:\n",
    "            predict = torch.sigmoid(output).round().byte().squeeze()\n",
    "        else:\n",
    "            predict = torch.argmax(output, axis=1).byte().squeeze()\n",
    "        return predict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.forward_batch(batch)\n",
    "        labels = self.process_labels(batch)\n",
    "        loss = self.loss_function(input=output, target=labels)\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        stage = self.trainer.state.stage\n",
    "        output = self.forward_batch(batch)\n",
    "        labels = self.process_labels(batch)\n",
    "        loss = self.loss_function(input=output, target=labels)\n",
    "        # Update statistics for metric computation\n",
    "\n",
    "        if len(labels.shape) == 1:\n",
    "            labels.unsqueeze_(-1)\n",
    "\n",
    "        is_binary_classification = self.num_classes <= 2\n",
    "        output_transforms = Compose(\n",
    "            [\n",
    "                ToTensor(),\n",
    "                Activations(sigmoid=is_binary_classification),\n",
    "                AsDiscrete(threshold=0.5 if is_binary_classification else None),\n",
    "                AsDiscrete(\n",
    "                    to_onehot=self.num_classes, argmax=not is_binary_classification\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        target_transforms = Compose(\n",
    "            [ToTensor(), AsDiscrete(to_onehot=self.num_classes)]\n",
    "        )\n",
    "\n",
    "        onehot_output = [output_transforms(item.cpu()) for item in output]\n",
    "        onehot_target = [target_transforms(item.cpu()) for item in labels]\n",
    "\n",
    "        self.metric(onehot_output, onehot_target)\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "        return {f\"{stage}_loss\": loss, \"num_items\": len(output)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        stage = self.trainer.state.stage\n",
    "        loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            loss += output[f\"{stage}_loss\"].sum().item()\n",
    "            num_items += output[\"num_items\"]\n",
    "        mean_accuracy = self.metric.aggregate()[0].item()\n",
    "        self.metric.reset()\n",
    "\n",
    "        mean_loss = torch.tensor(loss / num_items)\n",
    "        self.logger.experiment.add_scalar(\n",
    "            f\"{stage}/loss\", mean_loss, self.current_epoch\n",
    "        )\n",
    "        self.logger.experiment.add_scalar(\n",
    "            f\"{stage}/accuracy\",\n",
    "            mean_accuracy,\n",
    "            self.current_epoch,\n",
    "        )\n",
    "        self.log(f\"{stage}_accuracy\", mean_accuracy, prog_bar=True, logger=False)\n",
    "        if stage == \"validate\":\n",
    "            if mean_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = mean_accuracy\n",
    "                self.best_val_epoch = self.current_epoch\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039b529-d05c-4df0-844f-86adb16234f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model and Dataset Configuration\n",
    "\n",
    "Running the next cell shows the supported MONAI models with number of trainable parameters and MedMNIST datasets with number of labels, and number of items in the train, validation and test set.\n",
    "\n",
    "In the cell after that, specify the model and dataset to use in this notebook\n",
    "\n",
    "For demo purposes, we define a `sample_config` with default values for all networks. Not all networks support all parameters; unsupported parameters will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577966ea-fdc8-4050-ae22-bd6b16ad765f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_config = {\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 1,\n",
    "    \"num_classes\": 2,\n",
    "    \"spatial_dims\": 2,\n",
    "    \"block_config\": [6, 12, 8],\n",
    "}\n",
    "\n",
    "supported_models = [\"DenseNet\", \"SENet154\", \"SEResNet50\", \"SEResNext50\"]\n",
    "print(f\"Supported models: {supported_models}\")\n",
    "for model_name in supported_models:\n",
    "    model = MonaiModel(model_name, sample_config)\n",
    "    param_size = summarize(model).trainable_parameters / 1000 / 1000\n",
    "    print(f\"Trainable parameters: {param_size:.2f} M\")\n",
    "    del model\n",
    "print()\n",
    "\n",
    "supported_datasets = [\n",
    "    (ds_name)\n",
    "    for (ds_name, ds_info) in medmnist.INFO.items()\n",
    "    if ds_info[\"task\"] in [\"multi-class\", \"binary-class\"]\n",
    "]\n",
    "print(\"Supported datasets:\")\n",
    "for key, value in medmnist.INFO.items():\n",
    "    if key in supported_datasets:\n",
    "        print(key, value[\"task\"], f\"{len(value['label'])} labels, {value['n_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a3a54-8552-4693-aa67-fa482d36905b",
   "metadata": {},
   "source": [
    "Specify the MedMNIST dataset and MONAI model to use. SENet154 is a large model which will take a long time to train. For training on CPU, DenseNet is recommended, in combination with a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1239da3-7e1e-43ca-ab4e-1b9883f8ef3b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medmnist_dataset = \"breastmnist\"\n",
    "monai_model = \"DenseNet\"\n",
    "\n",
    "assert medmnist_dataset in supported_datasets\n",
    "assert getattr(nets, monai_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54f55e-d8ee-497e-ad87-7b70b57607a5",
   "metadata": {},
   "source": [
    "The input arguments for the MONAI model are taken from the dataset information. For example, the `spatial_dims` argument to a MONAI model is set to the `num_dims` value of the MedMNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef810880-98fb-48f8-b318-1b11acdd028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=19, medmnist_dataset=medmnist_dataset)\n",
    "data.setup()\n",
    "out_channels = (\n",
    "    data.dataset_train.num_classes if data.dataset_train.num_classes > 2 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39700f1-2285-4e61-9f8b-51b65135b622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set default values for MONAI models, based on the chosen medmnist_dataset.\n",
    "# Not all models use all these parameters. Unused parameters are discarded in the MonaiModel.\n",
    "# This makes it easier to test multiple models without creating custom configurations\n",
    "\n",
    "default_config = {\n",
    "    \"in_channels\": data.dataset_train.num_channels,\n",
    "    \"out_channels\": out_channels,\n",
    "    \"num_classes\": data.dataset_train.num_classes,\n",
    "    \"spatial_dims\": data.dataset_train.num_dims,\n",
    "    \"block_config\": [6, 12, 8],\n",
    "}\n",
    "print(default_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee80ca6-296b-4ab6-9e32-426aef3e96e1",
   "metadata": {},
   "source": [
    "### Dataset Info and Visualization\n",
    "\n",
    "Show MedMNIST information about the dataset, including description, labels and number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524723fb-6c35-4ab6-9932-56d308503e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "medmnist.INFO[medmnist_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d55737-e915-4e5d-9a03-8d390674e583",
   "metadata": {},
   "source": [
    "Show a random sample of 10 images to verify (as much as that is possible with images of this size) that the dataset looks correct. For 3D images, the middle slice of the image is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ebea1-465b-4708-855f-72307b3a65ad",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(data.dataset_train)), 10)\n",
    "data_subset = itemgetter(*indices)(data.dataset_train)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 6))\n",
    "plt.suptitle(medmnist_dataset)\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, annotation = data_subset[i]\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[0][14]\n",
    "    label = data.dataset_train.labels[str(annotation.short().item())]\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10463e82-dab2-43ae-8fee-1abf498f2a3e",
   "metadata": {},
   "source": [
    "### Show Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed134a0-a508-4d95-b7b8-701cf6b38a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monai_lightning_model = MonaiModel(monai_model=monai_model, config=default_config)\n",
    "summarize(monai_lightning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f7f9c-ea9c-48cb-b3c9-a172caa33736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Training\n",
    "\n",
    "Create a PyTorch Lightning [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) and call the `.fit()` method to start training. Set `USE_CUDA` to True to enable training on CUDA-enabled GPUs and adjust the other settings as needed. The settings below train the model for 5 epochs, log to TensorBoard, and save the best 3 checkpoints, where \"best\" is defined as highest accuracy. On CUDA, 16 bit training is enabled. This is not supported on CPU. `limit_train_batches` can be useful for large datasets. At the end of training, the total training duration will be displayed.\n",
    "\n",
    "See the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) for information on all parameters and settings.\n",
    "\n",
    "Uncomment the next cell to show a TensorBoard dashboard in the notebook. This will initially show no data. Click on the refresh button in the TensorBoard cell to show data during and after training.\n",
    "\n",
    "To cancel training, click the stop button in the Jupyter toolbar at the top of the notebook. That will stop gracefully: the best checkpoint until that point is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8029eb-7f08-4c50-8df1-b279ae8463b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=f\"{medmnist_dataset}_{monai_model.lower()}\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validate_accuracy\", mode=\"max\", save_top_k=3\n",
    ")\n",
    "# Ignore PyTorch Lightning warnings about possible improvements\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Consider increasing the value of the `num_workers` argument*\"\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", \".*smaller than the logging interval*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*has already been called*\")\n",
    "\n",
    "limit_train_batches = 1.0\n",
    "limit_val_batches = 1.0\n",
    "\n",
    "# For demonstration purposes, we limit the number of dataset items for large datasets to reduce \n",
    "# training time. For better accuracy, set limit_train_batches to 1.0 (the default). \n",
    "# if len(data.train_dataloader().dataset) > 2000:\n",
    "#     limit_train_batches = 1/(len(data.train_dataloader().dataset) / 2000)\n",
    "# if len(data.val_dataloader().dataset) > 5000:\n",
    "#     limit_val_batches = 0.5\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    gpus=1 if USE_CUDA else 0,\n",
    "    logger=logger,\n",
    "    precision=16 if USE_CUDA else 32,\n",
    "    limit_train_batches=limit_train_batches,\n",
    "    limit_val_batches=limit_val_batches,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    fast_dev_run=False,  # set to True to quickly test the Lightning model\n",
    ")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "try:\n",
    "    trainer.fit(model=monai_lightning_model, datamodule=data)\n",
    "finally:\n",
    "    end = datetime.datetime.now()\n",
    "    delta = dateutil.relativedelta.relativedelta(end, start)\n",
    "    print(\n",
    "        f\"Training duration: {delta.hours:02d}:{delta.minutes:02d}:{delta.seconds:02d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9ace0-7d7c-47f1-9a88-0117f27ba8a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate Trained Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ecd12-2c81-472e-a910-9cdd1645cb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monai_lightning_model.best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52ed76-79e1-48a8-9355-b4c3eb06ae5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_test_accuracy = trainer.test(ckpt_path=\"best\", dataloaders=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b3f29-1890-4509-9fc1-74aa33aa3f73",
   "metadata": {},
   "source": [
    "Visualize images from the test set and print actual and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda021e-a41b-42a0-82b4-ca2d9c6a8b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(data.dataset_test)), 10)\n",
    "data_subset = itemgetter(*indices)(data.dataset_test)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 6))\n",
    "plt.suptitle(f\"{monai_model}, {medmnist_dataset} labels/predictions\")\n",
    "monai_lightning_model.eval()\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, annotation = data_subset[i]\n",
    "    prediction = monai_lightning_model.predict_one(image.unsqueeze(0)).item()\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[0][14]\n",
    "    target_label = f\"label: {data.dataset_train.labels[str(annotation.short().item())]}\"\n",
    "    predicted_label = f\"prediction: {data.dataset_train.labels[str(prediction)]}\"\n",
    "\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(f\"{target_label}\\n{predicted_label}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cc564-6745-41a2-89d8-26252eda4b34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert FP32 model to ONNX and IR\n",
    "\n",
    "Load the best checkpoint and export the model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d666f-22cd-4382-aa19-c010f6b3afc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"nncf_models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "onnx_path = MODEL_DIR / f\"{monai_model}_{medmnist_dataset}_fp32.onnx\"\n",
    "checkpoint_path = checkpoint_callback.best_model_path\n",
    "best_model = MonaiModel.load_from_checkpoint(checkpoint_path).cpu().eval()\n",
    "\n",
    "if data.dataset_train.num_dims == 3:\n",
    "    input_shape = [1, data.dataset_train.num_channels, 28, 28, 28]\n",
    "else:\n",
    "    input_shape = [1, data.dataset_train.num_channels, 28, 28]\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(*input_shape)\n",
    "torch.onnx.export(best_model, dummy_input, onnx_path, opset_version=10)\n",
    "print(f\"Exported ONNX model to {onnx_path}\")\n",
    "\n",
    "# Convert ONNX model and export to OpenVINO IR\n",
    "fp_ir_path = onnx_path.with_suffix(\".xml\")\n",
    "IECore().read_network(onnx_path).serialize(str(fp_ir_path))\n",
    "print(f\"Exported FP32 IR model to {fp_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98752a-4583-4f51-a35a-f4f207982cbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NNCF\n",
    "\n",
    "Created an nncf compressed model requires a configuration dictionary, a dataloader, and a model. We use the `best_model` loaded in the previous cell, and use the validation dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0236c-c9bc-4f37-ad97-b97cc840c7da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NNCF_OUTPUT_DIR = Path(\"output\")\n",
    "NNCF_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "input_shape = list(next(iter(data.val_dataloader()))[0].shape)\n",
    "input_shape[0] = 1\n",
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": input_shape},\n",
    "    \"log_dir\": str(\n",
    "        NNCF_OUTPUT_DIR\n",
    "    ),  \n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",\n",
    "    },\n",
    "}\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)\n",
    "nncf_config = register_default_init_args(nncf_config, data.val_dataloader())\n",
    "\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages\n",
    "compression_ctrl, compressed_model = create_compressed_model(\n",
    "    best_model._model, nncf_config\n",
    ")\n",
    "del best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ba606-faff-441c-907c-34e019eef13f",
   "metadata": {},
   "source": [
    "### Export Quantized Model to ONNX and OpenVINO IR.\n",
    "\n",
    "The quantized model can be exported to ONNX with NNCF's `export_model` method. We then export to OpenVINO IR. The ONNX modelfile will have the same size as the PyTorch model file. The quantized IR model file will have a smaller size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3161b98-eeb6-49b9-be4e-4cad3b40dad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "int8_onnx_path = MODEL_DIR / f\"{monai_model}_{medmnist_dataset}_nncf.onnx\"\n",
    "compression_ctrl.export_model(int8_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a529f0-932c-4634-bff0-0081cd6b859f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Convert to OpenVINO IR\n",
    "\n",
    "We use Model Optimizer to convert the ONNX model to OpenVINO's Intermediate Representation (IR) format. In Jupyter we can call it with `! mo`, in a script we can use the subprocess module. We use subprocess here, to make it easy to convert this notebook to a script.\n",
    "\n",
    "For ONNX conversion, Model Optimizer only needs a path to an input model. We also specify an `output_dir` to save the model. Model Optimizer creates an .xml and .bin file, with the same base filename as the ONNX model. The .xml file contains information about the network topology, the .bin file contains weights and biases binary data. By default, weights and biases are stored as FP32. To convert them to FP16, set `--data_type` to FP16. This saves space (FP16 takes half as much space as FP32), and increases inference speed when using an Intel integrated GPU. \n",
    "\n",
    "Run `!mo --help` to see information about all Model Optimizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562e42e-d6d8-4e7b-9af5-fcae1cf09fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mo_result = subprocess.run(\n",
    "    [\"mo\", \"--input_model\", int8_onnx_path, \"--output_dir\", MODEL_DIR],\n",
    "    check=False,\n",
    "    universal_newlines=True,\n",
    "    capture_output=True,\n",
    ")\n",
    "if mo_result.returncode == 0:\n",
    "    print(\n",
    "        \"\\n\".join([line for line in mo_result.stdout.split(\"\\n\") if \"SUCCESS\" in line])\n",
    "    )\n",
    "else:\n",
    "    mo_error = \"\\n\".join([line for line in mo_result.stderr.split(\"\\n\")])\n",
    "    raise RuntimeError(\n",
    "        f\"Model optimization failed with the following error:\\n{mo_error}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f72df-55f0-41a3-ab66-3b8e8ae1ada0",
   "metadata": {},
   "source": [
    "Create a helper function to do inference on an IR model, and load the floating point and integer IR models to Inference Engine. Show inference results on the PyTorch Model, the floating point IR model, and the quantized INT8 IR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a0ec64-4349-4731-9e72-10bf944c518a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T14:37:32.063195Z",
     "iopub.status.busy": "2022-01-10T14:37:32.063076Z",
     "iopub.status.idle": "2022-01-10T14:37:32.064803Z",
     "shell.execute_reply": "2022-01-10T14:37:32.064526Z",
     "shell.execute_reply.started": "2022-01-10T14:37:32.063184Z"
    },
    "tags": []
   },
   "source": [
    "## Compare Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e6a78-fb15-4d56-a5ac-f6cb8fe942cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_ir(exec_net, image, return_class_index: bool=True):\n",
    "    \"\"\"\n",
    "    Do inference of image on exec_net. Return the result as class index integer\n",
    "    \"\"\"\n",
    "    input_layer = next(iter(exec_net.input_info))\n",
    "    output_layer = next(iter(exec_net.outputs))\n",
    "    output = exec_net.infer(inputs={input_layer: image})[output_layer]\n",
    "    if return_class_index:\n",
    "        output_shape = exec_net.outputs[output_layer].shape\n",
    "        if output_shape[-1] == 1:\n",
    "            predicted_class_index = sigmoid(output).round().astype(np.uint8).squeeze()\n",
    "        else:\n",
    "            predicted_class_index = np.argmax(output, axis=1).astype(np.uint8).squeeze()\n",
    "        result = predicted_class_index\n",
    "    else:\n",
    "        result = output\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6541b3-71fd-442f-a2e3-31bda03f434a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(exec_net, dataset, num_classes):\n",
    "    metric = ConfusionMatrixMetric(metric_name=\"accuracy\")\n",
    "    for image, label in dataset:\n",
    "\n",
    "        output = predict_ir(exec_net, image, return_class_index=False)\n",
    "\n",
    "        if len(label.shape) == 1:\n",
    "            label.unsqueeze_(-1)\n",
    "\n",
    "        is_binary_classification = num_classes <= 2\n",
    "        output_transforms = Compose(\n",
    "            [\n",
    "                ToTensor(),\n",
    "                Activations(sigmoid=is_binary_classification),\n",
    "                AsDiscrete(threshold=0.5 if is_binary_classification else None),\n",
    "                AsDiscrete(\n",
    "                    to_onehot=num_classes, argmax=not is_binary_classification\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        target_transforms = Compose(\n",
    "            [ToTensor(), AsDiscrete(to_onehot=num_classes)]\n",
    "        )\n",
    "\n",
    "        onehot_output = [output_transforms(item) for item in output]\n",
    "        onehot_target = [target_transforms(item) for item in label]\n",
    "\n",
    "        metric(onehot_output, onehot_target)\n",
    "    return metric.aggregate()[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357217b-ff28-4404-a27a-4ab0f60de098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load IR and PyTorch Lightning Models\n",
    "ie = IECore()\n",
    "int8_ir_path = int8_onnx_path.with_suffix(\".xml\")\n",
    "fp_net = ie.read_network(fp_ir_path)\n",
    "fp_exec_net = ie.load_network(fp_net, \"CPU\")\n",
    "int8_net = ie.read_network(int8_ir_path)\n",
    "int8_exec_net = ie.load_network(int8_net, \"CPU\")\n",
    "best_model = MonaiModel.load_from_checkpoint(checkpoint_path).cpu().eval()\n",
    "num_classes = monai_lightning_model.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff9919-2d3f-4ada-badc-e523986153ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_dataset = MedMNISTDataset(medmnist_dataset=medmnist_dataset, split=\"val\")\n",
    "fp_acc = compute_accuracy(fp_exec_net, validation_dataset, num_classes)\n",
    "int8_acc = compute_accuracy(int8_exec_net, validation_dataset, num_classes)\n",
    "\n",
    "print(f\"Validation accuracy of the PyTorch model: {monai_lightning_model.best_val_accuracy:.5f}\")\n",
    "print(f\"Validation accuracy of the original IR model: {fp_acc:.5f}\")\n",
    "print(f\"Validation accuracy of the quantized IR model: {int8_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a4858-a50a-4dc0-b333-6b08df58dc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = MedMNISTDataset(medmnist_dataset=medmnist_dataset, split=\"test\")\n",
    "fp_acc = compute_accuracy(fp_exec_net, test_dataset, num_classes)\n",
    "int8_acc = compute_accuracy(int8_exec_net, test_dataset, num_classes)\n",
    "\n",
    "print(f\"Test accuracy of the PyTorch model: {torch_test_accuracy[0]['test_accuracy']:.5f}\")\n",
    "print(f\"Test accuracy of the original IR model: {fp_acc:.5f}\")\n",
    "print(f\"Test accuracy of the quantized IR model: {int8_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab67f5-12a3-4354-aa40-26b0621fb86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T14:52:22.314372Z",
     "iopub.status.busy": "2022-01-10T14:52:22.314259Z",
     "iopub.status.idle": "2022-01-10T14:52:22.316128Z",
     "shell.execute_reply": "2022-01-10T14:52:22.315846Z",
     "shell.execute_reply.started": "2022-01-10T14:52:22.314362Z"
    }
   },
   "source": [
    "## Visually Compare Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7caf74-9ce2-42a8-8126-1ce547d7a569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "# Load 10 random images. Run this cell again to see inference results on\n",
    "# 10 different images\n",
    "indices = random.sample(range(len(data.dataset_test)), 10)\n",
    "data_subset = itemgetter(*indices)(data.dataset_test)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 6))\n",
    "fig.tight_layout(h_pad=5)\n",
    "monai_lightning_model.eval()\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, annotation = data_subset[i]\n",
    "    input_image = image.unsqueeze(0)\n",
    "    torch_prediction = best_model.predict_one(input_image).item()\n",
    "    fp_ir_prediction = predict_ir(fp_exec_net, input_image)\n",
    "    int8_ir_prediction = predict_ir(int8_exec_net, input_image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[0][14]\n",
    "    target_label = (\n",
    "        f\"annotation: {data.dataset_train.labels[str(annotation.short().item())]}\"\n",
    "    )\n",
    "    torch_predicted_label = f\"torch: {data.dataset_train.labels[str(torch_prediction)]}\"\n",
    "    fp_predicted_label = f\"FP IR: {data.dataset_train.labels[str(fp_ir_prediction)]}\"\n",
    "    int8_predicted_label = (\n",
    "        f\"INT8 IR: {data.dataset_train.labels[str(int8_ir_prediction)]}\"\n",
    "    )\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(\n",
    "        f\"{target_label}\\n{torch_predicted_label}\\n{fp_predicted_label}\\n{int8_predicted_label}\"\n",
    "    )\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4983ca-01f1-4867-a297-6f343c65b86c",
   "metadata": {},
   "source": [
    "## Benchmark Performance\n",
    "\n",
    "To measure the inference performance of the FP16 and INT8 models, we use Benchmark Tool, OpenVINO's inference performance measurement tool. Benchmark tool is a command line application that can be run in the notebook with ! benchmark_app or %sx benchmark_app.\n",
    "\n",
    "In the next cell, we create a wrapper function for `benchmark_app` that prints the benchmark_app command with the chosen parameters. For comparison purposes, it filters logging information from the output of `benchmark_app`.\n",
    "\n",
    "> NOTE: For the most accurate performance estimation, we recommended running benchmark_app in a terminal/command prompt after closing other applications. Run `benchmark_app --help` to see all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca87e49-d3d6-4502-8e4b-af64a06da3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def benchmark_model(\n",
    "    model_path: os.PathLike,\n",
    "    device: str = \"CPU\",\n",
    "    seconds: int = 60,\n",
    "    api: str = \"async\",\n",
    "    batch: int = 1,\n",
    "    cache_dir=\"model_cache\",\n",
    "    key=None\n",
    "):\n",
    "    ie = IECore()\n",
    "    model_path = Path(model_path)\n",
    "    if (\"GPU\" in device) and (\"GPU\" not in ie.available_devices):\n",
    "        raise ValueError(\n",
    "            f\"A GPU device is not available. Available devices are: {ie.available_devices}\"\n",
    "        )\n",
    "    else:\n",
    "        benchmark_command = f\"benchmark_app -m {model_path} -d {device} -t {seconds} -api {api} -b {batch} -cdir {cache_dir}\"\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"**Benchmark {model_path.name} with {device} for {seconds} seconds with {api} inference**\"\n",
    "            )\n",
    "        )\n",
    "        display(Markdown(f\"Benchmark command: `{benchmark_command}`\"))\n",
    "\n",
    "        benchmark_output = subprocess.run(\n",
    "            benchmark_command.split(\" \"), capture_output=True, universal_newlines=True\n",
    "        )\n",
    "        benchmark_result = [\n",
    "            line.replace(\"Latency\", f\"{key}_Latency\").replace(\"Throughput\", f\"{key}_Throughput\")\n",
    "            for line in benchmark_output.stdout.splitlines()\n",
    "            if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")\n",
    "        ]\n",
    "        print(\"\\n\".join(benchmark_result))\n",
    "        print()\n",
    "        if \"MULTI\" in device:\n",
    "            devices = device.replace(\"MULTI:\", \"\").split(\",\")\n",
    "            for single_device in devices:\n",
    "                print(\n",
    "                    f\"{single_device} device: {ie.get_metric(device_name=single_device, metric_name='FULL_DEVICE_NAME')}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                f\"Device: {ie.get_metric(device_name=device, metric_name='FULL_DEVICE_NAME')}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24802c7-6614-4516-83ef-df18a86100dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FP model on CPU\n",
    "benchmark_model(fp_ir_path, device=\"CPU\", seconds=15, api=\"sync\", key=\"FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafa3cd-9751-42c4-869d-e9560d200a40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INT8 model on CPU\n",
    "benchmark_model(int8_ir_path, device=\"CPU\", seconds=15, api=\"sync\", key=\"INT8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
