{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871269e-d4de-47fc-9485-ef91f3daa66f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Training, Optimization and Quantization with MONAI, PyTorch Lightning and OpenVINO\n",
    "\n",
    "This tutorial shows how to train a [MONAI](https://monai.io/) classification model on a [MedMNIST](https://medmnist.com/) dataset, and quantize the model with [OpenVINO's Post-Training Optimization Tool](https://docs.openvino.ai/latest/pot_README.html)\n",
    "\n",
    "To run this notebook, please create a virtual environment with Python 3.7 or 3.8, with `python -m venv monai_env` (on Linux use `python3`) and install the requirements with `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2dfd8-e5fa-4439-9862-3b48dc4082eb",
   "metadata": {},
   "source": [
    "## Supported Networks and Datasets\n",
    "\n",
    "The following MONAI Classification Networks are supported in this notebook:\n",
    "\n",
    "`[\"DenseNet\",\"SENet154\", \"SEResNet50\",  \"SEResNext50\"]`. \n",
    "\n",
    "The variants of these networks `\"DenseNet121\", \"DenseNet169\", \"DenseNet201\", \"DenseNet264\",  \"SEResNet101\", \"SEResNet152\", \"SEResNext101\"` also work. \n",
    "\n",
    "All MedMNIST datasets for multi-class and binary-class classification are supported: `['pathmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist', 'organmnist3d', 'nodulemnist3d', 'adrenalmnist3d', 'fracturemnist3d', 'vesselmnist3d', 'synapsemnist3d']`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0ff1e-08fa-4e3f-a4d9-7f0d65df2b91",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db1c72-8d54-4192-8ffa-ee2ebab67c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import inspect\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import addict\n",
    "import dateutil\n",
    "import matplotlib.pyplot as plt\n",
    "import medmnist\n",
    "import monai\n",
    "import monai.networks.nets as nets\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from compression.api import DataLoader, Metric\n",
    "from compression.engines.ie_engine import IEEngine\n",
    "from compression.graph import load_model, save_model\n",
    "from compression.graph.model_utils import compress_model_weights\n",
    "from compression.pipeline.initializer import create_pipeline\n",
    "from IPython.display import Markdown, display\n",
    "from monai.transforms import AddChannel, AsChannelFirst, Compose, ToTensor\n",
    "from openvino.inference_engine import IECore\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.model_summary import summarize\n",
    "from torch.utils.data import DataLoader as TorchDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e314f-3b41-48ca-877d-ee2e21bcbbfb",
   "metadata": {},
   "source": [
    "## Dataset, Metric and Model\n",
    "\n",
    "We start by defining a Dataset and DataModule to handle data transformation and loading, a Metric class that specifies how to evaluate the model, and the PyTorch Lightning Model that specifies the MONAI model to use and contains the training and evaluation code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4a936-87fa-475c-af5e-dd1042de4751",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "We create a Monai Dataset to load and transform the data, and a PyTorch Lighnting DataModule for accessing this data during training. The dataset returns data as a tuple consisting of (image, mask, image_metadata, mask_metadata).\n",
    "\n",
    "We use Monai Transforms to transform and augment the data during training. During training, we randomly rotate the data, add noise, and shift pixel values. Monai's ImageDataset ensures that the random seed for the image and segmentation mask transform are the same, and therefore that for the random rotation transforms, image and mask will be rotated in the same way. During validation, we only make sure that the dimensions and data type are correct.\n",
    "\n",
    "The specified MedMNIST dataset is downloaded if it has not been downloaded before. See the top of this notebook for the supported datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1586697-6d7a-4f45-ad8e-868c04d1f396",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fead4-d2d4-49e2-9dff-fd9f42c9bff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedMNISTDataset(DataLoader):\n",
    "    def __init__(self, medmnist_dataset, split) -> None:\n",
    "\n",
    "        supported_datasets = [\n",
    "            (ds_name)\n",
    "            for (ds_name, ds_info) in medmnist.INFO.items()\n",
    "            if ds_info[\"task\"] in [\"multi-class\", \"binary-class\"]\n",
    "        ]\n",
    "\n",
    "        if medmnist_dataset not in supported_datasets:\n",
    "            raise ValueError(\n",
    "                f\"{medmnist_dataset} is not a supported dataset. Supported datasets are: \"\n",
    "                f\"{supported_datasets}.\"\n",
    "            )\n",
    "\n",
    "        dataset = getattr(\n",
    "            medmnist, medmnist.dataset.INFO[medmnist_dataset][\"python_class\"]\n",
    "        )\n",
    "        self.num_dims = 3 if medmnist_dataset.endswith(\"3d\") else 2\n",
    "        self.num_channels = medmnist.dataset.INFO[medmnist_dataset][\"n_channels\"]\n",
    "        self.labels = medmnist.dataset.INFO[medmnist_dataset][\"label\"]\n",
    "        self.num_classes = len(self.labels)\n",
    "\n",
    "        transforms = [ToTensor(dtype=torch.float)]\n",
    "        # transforms = []\n",
    "        if self.num_channels == 3:\n",
    "            transforms.append(AsChannelFirst())\n",
    "        elif self.num_dims == 2:\n",
    "            transforms.append(AddChannel())\n",
    "\n",
    "        transform = Compose(transforms)\n",
    "\n",
    "        print(\n",
    "            f\"Setup {medmnist_dataset} {split}, {self.num_channels} channels, \"\n",
    "            f\"{self.num_classes} classes\"\n",
    "        )\n",
    "        split_dataset = dataset(split=split, transform=None, download=True)\n",
    "\n",
    "        data = [\n",
    "            (transform(np.asarray(item[0])), torch.as_tensor(item[1]))\n",
    "            for item in split_dataset\n",
    "        ]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of elements in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get item from self.dataset at the specified index.\n",
    "        Returns (annotation, image), where annotation is a tuple (index, class_index)\n",
    "        and image a preprocessed image in network shape\n",
    "        \"\"\"\n",
    "        image, label = self.data[index]\n",
    "        image = torch.as_tensor(image, dtype=torch.float)\n",
    "        label = torch.as_tensor(label, dtype=torch.float)\n",
    "        # image = self.transform(image)\n",
    "        annotation = (index, label)\n",
    "        return annotation, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c94bc-d36d-4771-bc5b-eb1cbea6c33b",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning DataModule\n",
    "\n",
    "A [Lightning DataModule]( https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html) defines dataloaders and connects the Lightning Module to the Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e9fa6-3ec8-47b4-9c52-b36361e02753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, medmnist_dataset, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.medmnist_dataset = medmnist_dataset\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        random.seed(1.414213)\n",
    "\n",
    "        self.dataset_train = MedMNISTDataset(self.medmnist_dataset, \"train\")\n",
    "        self.dataset_val = MedMNISTDataset(self.medmnist_dataset, \"val\")\n",
    "        self.dataset_test = MedMNISTDataset(self.medmnist_dataset, \"test\")\n",
    "\n",
    "        print(f\"Setup train dataset: {len(self.dataset_train)} items\")\n",
    "        print(f\"Setup val dataset: {len(self.dataset_val)} items\")\n",
    "        print(f\"Setup test dataset: {len(self.dataset_test)} items\")\n",
    "\n",
    "        assert len(self.dataset_train) > 0, \"Train dataset is empty.\"\n",
    "        assert len(self.dataset_val) > 0, \"Val dataset is empty\"\n",
    "        assert len(self.dataset_test) > 0, \"Val dataset is empty\"\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return TorchDataLoader(\n",
    "            self.dataset_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f720cb-ce65-4416-ab9b-04951765054c",
   "metadata": {},
   "source": [
    "### Accuracy Metric\n",
    "\n",
    "The Accuracy class implements POT's Metric interface. This class will be used during training to keep track of validation accuracy, and for POT quantization, to compare the accuracy of the FP32 model with that of the INT8 model. It can also be used for Accuracy Aware quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b2631-1af7-4c29-b263-fab63306675c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "def sigmoid(x):\n",
    "    # if isinstance(x, torch.Tensor):\n",
    "    #     return torch.sigmoid(x).numpy()\n",
    "    # else:\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "class Accuracy(Metric):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self._name = \"accuracy\"\n",
    "        self._matches = []\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Returns accuracy metric value for the last model output.\"\"\"\n",
    "        return {self._name: [self._matches[-1]]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"\n",
    "        Returns accuracy metric value for all model outputs. Results per image are stored in\n",
    "        self._matches, where True means a correct prediction and False a wrong prediction.\n",
    "        Accuracy is computed as the number of correct predictions divided by the total\n",
    "        number of predictions.\n",
    "        \"\"\"\n",
    "        num_correct = np.count_nonzero(self._matches)\n",
    "        return {self._name: num_correct / len(self._matches)}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"\n",
    "        Updates prediction matches.\n",
    "\n",
    "        :param output: model output\n",
    "        :param target: annotations\n",
    "        \"\"\"\n",
    "        if isinstance(output, list):\n",
    "            output = output[0]\n",
    "            target = target[0]\n",
    "        if self.num_classes <= 2:\n",
    "            predict = sigmoid(output).round()\n",
    "        else:\n",
    "            predict = np.argmax(output, axis=1)\n",
    "\n",
    "        for p_item, t_item in zip(predict, target):\n",
    "            match = p_item.item() == t_item.item()\n",
    "            if isinstance(match, (np.ndarray, torch.Tensor)):\n",
    "                self._matches += match.tolist()\n",
    "            else:\n",
    "                self._matches.append(match)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the Accuracy metric. This is a required method that should initialize all\n",
    "        attributes to their initial value.\n",
    "        \"\"\"\n",
    "        self._matches = []\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                             'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._name: {\"direction\": \"higher-better\", \"type\": \"accuracy\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59d94f-8e66-450c-8b43-d51daa68c098",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We create a PyTorch Lightning [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) to train a [Monai network](https://docs.monai.io/en/latest/networks.html#nets)\n",
    "\n",
    "For binary classification models we use Binary Cross Entropy Loss, for multiclass segmentation Cross Entropy Loss. For optimizer, we use the Adam Optimizer with the default learning rate of 0.001. The evaluation metric is accuracy, implemented with the Accuracy class defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247110f4-9b80-4fdb-bccc-2448e4d0e833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MonaiModel(pl.LightningModule):\n",
    "    def __init__(self, monai_model: str, config: Dict):\n",
    "        \"\"\"\n",
    "        PyTorch Lightning Module for a given MONAI classification model.\n",
    "\n",
    "        :param monai_model: MONAI model name. For example, DenseNet, SEResNet50\n",
    "        :param config: Dictionary with configuration values to pass to MONAI model initialization.\n",
    "                       Dictionary keys that are not supported model parameters are discarded.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        model = getattr(nets, monai_model)\n",
    "        model_config = config.copy()\n",
    "\n",
    "        for parameter in model_config.copy():\n",
    "            if (\n",
    "                parameter not in inspect.signature(model).parameters\n",
    "                and parameter not in inspect.signature(model.__bases__[0]).parameters\n",
    "            ):\n",
    "                model_config.pop(parameter)\n",
    "        self._model = model(**model_config).cpu()\n",
    "        self.num_classes = model_config.get(\"num_classes\") or model_config.get(\n",
    "            \"out_channels\"\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        # https://docs.monai.io/en/latest/highlights.html?deterministic-training-for-reproducibility\n",
    "        monai.utils.set_determinism(seed=2.71828, additional_settings=None)\n",
    "        self.loss_function = (\n",
    "            torch.nn.CrossEntropyLoss()\n",
    "            if self.num_classes > 2\n",
    "            else torch.nn.BCEWithLogitsLoss()\n",
    "        )\n",
    "        self.metric = Accuracy(num_classes=self.num_classes)\n",
    "        self.best_val_accuracy = 0\n",
    "        self.best_val_epoch = 0\n",
    "        print(f\"Initialized {monai_model} with settings: {model_config}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def forward_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Propagate images through the network\n",
    "\n",
    "        :return: raw network output in layout expected by loss function\n",
    "        \"\"\"\n",
    "        _, images = batch\n",
    "        images = torch.as_tensor(images, dtype=torch.float)\n",
    "        output = self.forward(images)\n",
    "        if isinstance(self.loss_function, torch.nn.BCEWithLogitsLoss):\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.unsqueeze(-1)\n",
    "        return output\n",
    "\n",
    "    def process_labels(self, batch):\n",
    "        \"\"\"\n",
    "        Return labels in format expected by the loss function (expected to be\n",
    "        BCEWithLogitsLoss for binary classification and CrossEntropyLoss for multiclass\n",
    "\n",
    "        :return: labels in correct datatype and layout\n",
    "        \"\"\"\n",
    "        annotation, _ = batch\n",
    "        labels = torch.as_tensor(annotation[1], dtype=torch.float)\n",
    "        if isinstance(self.loss_function, torch.nn.BCEWithLogitsLoss):\n",
    "            labels = labels.float()\n",
    "        else:\n",
    "            labels = labels.long().squeeze(dim=1)\n",
    "        return labels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_one(self, x):\n",
    "        \"\"\"\n",
    "        Propage one image through the network and return the result as a class index\n",
    "        Uses sigmoid for binary classification and argmax for multiclass\n",
    "\n",
    "        :return prediction as class index integer\n",
    "        \"\"\"\n",
    "        output = self.forward(x)\n",
    "        if self.num_classes <= 2:\n",
    "            predict = torch.sigmoid(output).round().byte().squeeze()\n",
    "        else:\n",
    "            predict = torch.argmax(output, axis=1).byte().squeeze()\n",
    "        return predict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.forward_batch(batch)\n",
    "        labels = self.process_labels(batch)\n",
    "        loss = self.loss_function(input=output, target=labels)\n",
    "        self.log(\"train_loss\", loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        stage = self.trainer.state.stage\n",
    "        output = self.forward_batch(batch)\n",
    "        labels = self.process_labels(batch)\n",
    "        loss = self.loss_function(input=output, target=labels)\n",
    "        # Update statistics for metric computation\n",
    "        self.metric.update(output=output.cpu(), target=labels.long().cpu())\n",
    "        self.log(f\"{stage}_loss\", loss)\n",
    "        return {f\"{stage}_loss\": loss, \"num_items\": len(output)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        stage = self.trainer.state.stage\n",
    "        loss, num_items = 0, 0\n",
    "        for output in outputs:\n",
    "            loss += output[f\"{stage}_loss\"].sum().item()\n",
    "            num_items += output[\"num_items\"]\n",
    "        mean_accuracy = self.metric.avg_value[self.metric._name]\n",
    "        self.metric.reset()\n",
    "\n",
    "        mean_loss = torch.tensor(loss / num_items)\n",
    "        self.logger.experiment.add_scalar(\n",
    "            f\"{stage}/loss\", mean_loss, self.current_epoch\n",
    "        )\n",
    "        self.logger.experiment.add_scalar(\n",
    "            f\"{stage}/accuracy\",\n",
    "            mean_accuracy,\n",
    "            self.current_epoch,\n",
    "        )\n",
    "        self.log(f\"{stage}_accuracy\", mean_accuracy, prog_bar=True, logger=False)\n",
    "        if stage == \"validate\":\n",
    "            if mean_accuracy > self.best_val_accuracy:\n",
    "                self.best_val_accuracy = mean_accuracy\n",
    "                self.best_val_epoch = self.current_epoch\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039b529-d05c-4df0-844f-86adb16234f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model and Dataset Configuration\n",
    "\n",
    "For demo purposes, we define a `sample_config` with default values for all networks. Not all networks support all parameters, in that case they will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d82a8-59aa-42f8-b916-73b0d649ac67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_config = {\n",
    "    \"in_channels\": 1,\n",
    "    \"out_channels\": 1,\n",
    "    \"num_classes\": 1,\n",
    "    \"spatial_dims\": 2,\n",
    "    \"block_config\": [6, 12, 8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51fe9d2-9994-400d-ad04-fa1f966b7b59",
   "metadata": {},
   "source": [
    "Running the next cell shows the supported MONAI models with number of trainable parameters and MedMNIST datasets with number of labels, and number of items in the train, validation and test set.\n",
    "\n",
    "In the cell after that, specify the model and dataset to use in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a36386-6857-44a1-8f59-04010c547c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "supported_models = [\"DenseNet\", \"SENet154\", \"SEResNet50\", \"SEResNext50\"]\n",
    "print(f\"Supported models: {supported_models}\")\n",
    "for model_name in supported_models:\n",
    "    model = MonaiModel(model_name, sample_config)\n",
    "    param_size = summarize(model).trainable_parameters / 1000 / 1000\n",
    "    print(f\"Trainable parameters: {param_size:.2f} M\")\n",
    "print()\n",
    "\n",
    "supported_datasets = [\n",
    "    (ds_name)\n",
    "    for (ds_name, ds_info) in medmnist.INFO.items()\n",
    "    if ds_info[\"task\"] in [\"multi-class\", \"binary-class\"]\n",
    "]\n",
    "print(\"Supported datasets:\")\n",
    "for key, value in medmnist.INFO.items():\n",
    "    if key in supported_datasets:\n",
    "        print(key, value[\"task\"], f\"{len(value['label'])} labels, {value['n_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a3a54-8552-4693-aa67-fa482d36905b",
   "metadata": {},
   "source": [
    "Specify the MedMNIST dataset and MONAI model to use. SENet154 is a large model which will take a long time to train. For training on CPU, DenseNet is recommended, in combination with a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1239da3-7e1e-43ca-ab4e-1b9883f8ef3b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "medmnist_dataset = \"breastmnist\"\n",
    "monai_model = \"DenseNet\"\n",
    "\n",
    "assert medmnist_dataset in supported_datasets\n",
    "assert getattr(nets, monai_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54f55e-d8ee-497e-ad87-7b70b57607a5",
   "metadata": {},
   "source": [
    "The input arguments for the MONAI model are taken from the dataset information. For example, the `spatial_dims` argument to a MONAI model is set to the `num_dims` value of the MedMNIST dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef810880-98fb-48f8-b318-1b11acdd028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = DataModule(batch_size=19, medmnist_dataset=medmnist_dataset)\n",
    "data.setup()\n",
    "out_channels = (\n",
    "    data.dataset_train.num_classes if data.dataset_train.num_classes > 2 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39700f1-2285-4e61-9f8b-51b65135b622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set default values for MONAI models, based on the chosen medmnist_dataset.\n",
    "# Not all models use all these parameters. Unused parameters are discarded in the MonaiModel.\n",
    "# This makes it easier to test multiple models without creating custom configurations\n",
    "\n",
    "default_config = {\n",
    "    \"in_channels\": data.dataset_train.num_channels,\n",
    "    \"out_channels\": out_channels,\n",
    "    \"num_classes\": out_channels,\n",
    "    \"spatial_dims\": data.dataset_train.num_dims,\n",
    "    \"block_config\": [6, 12, 8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee80ca6-296b-4ab6-9e32-426aef3e96e1",
   "metadata": {},
   "source": [
    "### Dataset Info and Visualization\n",
    "\n",
    "Show MedMNIST information about the dataset, including description, labels and number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524723fb-6c35-4ab6-9932-56d308503e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "medmnist.INFO[medmnist_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d55737-e915-4e5d-9a03-8d390674e583",
   "metadata": {},
   "source": [
    "Show a random sample of 10 images to verify (as much as that is possible with images of this size) that the dataset looks correct. For 3D images, the middle slice of the image is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ebea1-465b-4708-855f-72307b3a65ad",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(data.dataset_train)), 10)\n",
    "data_subset = itemgetter(*indices)(data.dataset_train)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 6))\n",
    "plt.suptitle(medmnist_dataset)\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    annotation, image = data_subset[i]\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[0][14]\n",
    "    label = data.dataset_train.labels[str(annotation[1].short().item())]\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10463e82-dab2-43ae-8fee-1abf498f2a3e",
   "metadata": {},
   "source": [
    "### Show Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed134a0-a508-4d95-b7b8-701cf6b38a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monai_lightning_model = MonaiModel(monai_model=monai_model, config=default_config)\n",
    "summarize(monai_lightning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f7f9c-ea9c-48cb-b3c9-a172caa33736",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Training\n",
    "\n",
    "Create a PyTorch Lightning [Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) and call the `.fit()` method to start training. Set `USE_CUDA` to True to enable training on CUDA-enabled GPUs and adjust the other settings as needed. The settings below train the model for 5 epochs, log to TensorBoard, and save the best 3 checkpoints, where \"best\" is defined as highest accuracy. On CUDA, 16 bit training is enabled. This is not supported on CPU. `limit_train_batches` can be useful for large datasets. At the end of training, the total training duration will be displayed.\n",
    "\n",
    "See the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) for information on all parameters and settings.\n",
    "\n",
    "Uncomment the next cell to show a TensorBoard dashboard in the notebook. This will initially show no data. Click on the refresh button in the TensorBoard cell to show data during and after training.\n",
    "\n",
    "To cancel training, click the stop button in the Jupyter toolbar at the top of the notebook. That will stop gracefully: the best checkpoint until that point is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8029eb-7f08-4c50-8df1-b279ae8463b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"tb_logs\", name=f\"{medmnist_dataset}_{monai_model.lower()}\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"validate_accuracy\", mode=\"max\", save_top_k=3\n",
    ")\n",
    "# Ignore PyTorch Lightning warnings about possible improvements\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Consider increasing the value of the `num_workers` argument*\"\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", \".*smaller than the logging interval*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*has already been called*\")\n",
    "\n",
    "USE_CUDA = False\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    gpus=1 if USE_CUDA else 0,\n",
    "    logger=logger,\n",
    "    precision=16 if USE_CUDA else 32,\n",
    "    # limit_train_batches=0.5,\n",
    "    # limit_val_batches=0.5,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    fast_dev_run=False,  # set to True to quickly test the Lightning model\n",
    ")\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "try:\n",
    "    trainer.fit(model=monai_lightning_model, datamodule=data)\n",
    "finally:\n",
    "    end = datetime.datetime.now()\n",
    "    delta = dateutil.relativedelta.relativedelta(end, start)\n",
    "    print(\n",
    "        f\"Training duration: {delta.hours:02d}:{delta.minutes:02d}:{delta.seconds:02d}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9ace0-7d7c-47f1-9a88-0117f27ba8a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate Trained Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52ed76-79e1-48a8-9355-b4c3eb06ae5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_test_accuracy = trainer.test(ckpt_path=\"best\", dataloaders=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b3f29-1890-4509-9fc1-74aa33aa3f73",
   "metadata": {},
   "source": [
    "Visualize images from the test set and print actual and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda021e-a41b-42a0-82b4-ca2d9c6a8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.sample(range(len(data.dataset_test)), 10)\n",
    "data_subset = itemgetter(*indices)(data.dataset_test)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 6))\n",
    "plt.suptitle(f\"{monai_model}, {medmnist_dataset} labels/predictions\")\n",
    "monai_lightning_model.eval()\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    annotation, image = data_subset[i]\n",
    "    prediction = monai_lightning_model.predict_one(image.unsqueeze(0)).item()\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[0][14]\n",
    "    target_label = (\n",
    "        f\"label: {data.dataset_train.labels[str(annotation[1].short().item())]}\"\n",
    "    )\n",
    "    predicted_label = f\"prediction: {data.dataset_train.labels[str(prediction)]}\"\n",
    "\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(f\"{target_label}\\n{predicted_label}\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98752a-4583-4f51-a35a-f4f207982cbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert to ONNX\n",
    "\n",
    "Load the best checkpoint and export the model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7951af-353e-439c-abde-23803761614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "onnx_path = MODEL_DIR / f\"{monai_model}_{medmnist_dataset}.onnx\"\n",
    "checkpoint_path = checkpoint_callback.best_model_path\n",
    "best_model = MonaiModel.load_from_checkpoint(checkpoint_path).cpu().eval()\n",
    "\n",
    "if data.dataset_train.num_dims == 3:\n",
    "    input_shape = [1, data.dataset_train.num_channels, 28, 28, 28]\n",
    "else:\n",
    "    input_shape = [1, data.dataset_train.num_channels, 28, 28]\n",
    "\n",
    "\n",
    "dummy_input = torch.randn(*input_shape)\n",
    "torch.onnx.export(best_model, dummy_input, onnx_path, opset_version=10)\n",
    "del best_model\n",
    "print(f\"Exported ONNX model to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a529f0-932c-4634-bff0-0081cd6b859f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert to OpenVINO IR\n",
    "\n",
    "We use Model Optimizer to convert the ONNX model to OpenVINO's Intermediate Representation (IR) format. In Jupyter we can call it with `! mo`, in a script we can use the subprocess module. We use subprocess here, to make it easy to convert this notebook to a script.\n",
    "\n",
    "For ONNX conversion, Model Optimizer only needs a path to an input model. We also specify an `output_dir` to save the model. Model Optimizer creates an .xml and .bin file, with the same base filename as the ONNX model. The .xml file contains information about the network topology, the .bin file contains weights and biases binary data. By default, weights and biases are stored as FP32. To convert them to FP16, set `--data_type` to FP16. This saves space (FP16 takes half as much space as FP32), and increases inference speed when using an Intel integrated GPU. \n",
    "\n",
    "Run `!mo --help` to see information about all Model Optimizer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562e42e-d6d8-4e7b-9af5-fcae1cf09fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !mo --input_model $onnx_path\n",
    "\n",
    "mo_result = subprocess.run(\n",
    "    [\"mo\", \"--input_model\", onnx_path, \"--output_dir\", MODEL_DIR],\n",
    "    check=False,\n",
    "    universal_newlines=True,\n",
    "    capture_output=True,\n",
    ")\n",
    "if mo_result.returncode == 0:\n",
    "    print(\n",
    "        \"\\n\".join([line for line in mo_result.stdout.split(\"\\n\") if \"SUCCESS\" in line])\n",
    "    )\n",
    "else:\n",
    "    mo_error = \"\\n\".join([line for line in mo_result.stderr.split(\"\\n\")])\n",
    "    raise RuntimeError(f\"Model optimization failed with the following error:\\n{mo_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7c3ab-c3a2-42eb-b57b-61b8395acd51",
   "metadata": {},
   "source": [
    "## Quantize with Post-Training Optimization Tool\n",
    "\n",
    "We quantize the model with OpenVINO's Post-Training Optimization Tool (POT) to increase inference speed.\n",
    "\n",
    "The Post-Training Optimization Tool (POT) compression API defines base classes for Metric and DataLoader. In the beinning of this notebook, we defined the Accuracy Metric and MedMNISTDataset DataLoader classes. These classes were used in the PyTorch Lightning training loop, and can also be used for POT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb5109-9a51-45d5-b4e2-b1c04ab57caf",
   "metadata": {},
   "source": [
    "### POT Config\n",
    "\n",
    "In the next cell, the settings for running quantization are defined. The default settings use the mixed preset and the DefaultQuantization algorithm. This enables reasonably fast quantization, with possible drop in accuracy. To use AccuracyAware quantization, set the algorithm name to AccuracyAwareQuantization.\n",
    "\n",
    "See [Post-Training Optimization Best Practices](https://docs.openvino.ai/latest/pot_docs_BestPractices.html) for information about the parameters and best practices for post-training quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771da177-28d7-47da-82db-c39fd90abc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp_ir_path = onnx_path.with_suffix(\".xml\")\n",
    "quantizationmethod = \"DefaultQuantization\"\n",
    "\n",
    "model_config = addict.Dict(\n",
    "    {\n",
    "        \"model_name\": fp_ir_path.stem,\n",
    "        \"model\": fp_ir_path,\n",
    "        \"weights\": fp_ir_path.with_suffix(\".bin\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "engine_config = addict.Dict(\n",
    "    {\"device\": \"CPU\", \"stat_requests_number\": 2, \"eval_requests_number\": 2}\n",
    ")\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": quantizationmethod,\n",
    "        \"params\": {\n",
    "            \"target_device\": \"CPU\",\n",
    "            \"preset\": \"mixed\",\n",
    "            \"stat_subset_size\": 300,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ab1a1-c47a-4cf5-8a0e-0f7dd6f975a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Execute POT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cad164-97b5-42b3-92b3-d984939a19ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Load the model\n",
    "model = load_model(model_config=model_config)\n",
    "original_model = copy.deepcopy(model)\n",
    "\n",
    "# Step 2: Initialize the data loader\n",
    "data_loader = data.dataset_val  # ClassificationDataLoader(dataset=data.dataset_val)\n",
    "\n",
    "# Step 3 (Optional. Required for AccuracyAwareQuantization): Initialize the metric\n",
    "#        Compute metric results on original model\n",
    "metric = Accuracy(num_classes=data.dataset_train.num_classes)\n",
    "\n",
    "# Step 4: Initialize the engine for metric calculation and statistics collection\n",
    "engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)\n",
    "\n",
    "# Step 5: Create a pipeline of compression algorithms\n",
    "pipeline = create_pipeline(algo_config=algorithms, engine=engine)\n",
    "\n",
    "# Step 6: Execute the pipeline\n",
    "compressed_model = pipeline.run(model=model)\n",
    "\n",
    "# Step 7 (Optional): Compress model weights quantized precision\n",
    "#                    in order to reduce the size of final .bin file\n",
    "compress_model_weights(model=compressed_model)\n",
    "\n",
    "# Step 8: Save the compressed model and get the path to the model\n",
    "method_suffix = \"def\" if quantizationmethod == \"DefaultQuantization\" else \"acc\"\n",
    "compressed_model_paths = save_model(\n",
    "    model=compressed_model,\n",
    "    model_name=f\"{model.name}_quantized_{method_suffix}\",\n",
    "    save_path=MODEL_DIR,\n",
    ")\n",
    "int8_ir_path = Path(compressed_model_paths[0][\"model\"])\n",
    "print(f\"The quantized model is stored in {int8_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de967a9-41c6-4c01-81c6-6a9e15fd3823",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "\n",
    "### Compare Accuracy\n",
    "\n",
    "Compare the accuracy of the original FP32 or FP16 model on the test dataset with the accuracy of the quantized INT8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3249e-4418-4875-abb9-3f64c032b4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Accuracy of the PyTorch model: {torch_test_accuracy[0]['test_accuracy']:.5f}\")\n",
    "\n",
    "test_engine = IEEngine(\n",
    "    config=engine_config, data_loader=data.dataset_test, metric=metric\n",
    ")\n",
    "test_pipeline = create_pipeline(algo_config=algorithms, engine=test_engine)\n",
    "\n",
    "original_metric_results = test_pipeline.evaluate(original_model)\n",
    "if original_metric_results:\n",
    "    print(\n",
    "        f\"Accuracy of the original IR model: {next(iter(original_metric_results.values())):.5f}\"\n",
    "    )\n",
    "\n",
    "quantized_metric_results = test_pipeline.evaluate(compressed_model)\n",
    "if quantized_metric_results:\n",
    "    print(\n",
    "        f\"Accuracy of the quantized IR model: {next(iter(quantized_metric_results.values())):.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1425e19-439a-4582-848f-d9c21be6e718",
   "metadata": {},
   "source": [
    "### Visually Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f72df-55f0-41a3-ab66-3b8e8ae1ada0",
   "metadata": {},
   "source": [
    "Create a helper function to do inference on an IR model, and load the floating point and integer IR models to Inference Engine. Show inference results on the PyTorch Model, the floating point IR model, and the quantized INT8 IR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e6a78-fb15-4d56-a5ac-f6cb8fe942cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ir(exec_net, image):\n",
    "    \"\"\"\n",
    "    Do inference of image on exec_net. Return the result as class index integer\n",
    "    \"\"\"\n",
    "    input_layer = next(iter(exec_net.input_info))\n",
    "    output_layer = next(iter(exec_net.outputs))\n",
    "    output = exec_net.infer(inputs={input_layer: image})[output_layer]\n",
    "    output_shape = exec_net.outputs[output_layer].shape\n",
    "    if output_shape[-1] == 1:\n",
    "        predicted_class_index = sigmoid(output).round().astype(np.uint8).squeeze()\n",
    "    else:\n",
    "        predicted_class_index = np.argmax(output, axis=1).astype(np.uint8).squeeze()\n",
    "    return predicted_class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357217b-ff28-4404-a27a-4ab0f60de098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IR and PyTorch Lightning Models\n",
    "ie = IECore()\n",
    "fp_net = ie.read_network(fp_ir_path)\n",
    "fp_exec_net = ie.load_network(fp_net, \"CPU\")\n",
    "int8_net = ie.read_network(int8_ir_path)\n",
    "int8_exec_net = ie.load_network(int8_net, \"CPU\")\n",
    "best_model = MonaiModel.load_from_checkpoint(checkpoint_path).cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7caf74-9ce2-42a8-8126-1ce547d7a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 10 random images. Run this cell again to see inference results on\n",
    "# 10 different images\n",
    "indices = random.sample(range(len(data.dataset_test)), 10)\n",
    "data_subset = itemgetter(*indices)(data.dataset_test)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 6))\n",
    "fig.tight_layout(h_pad=5)\n",
    "monai_lightning_model.eval()\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    annotation, image = data_subset[i]\n",
    "    input_image = image.unsqueeze(0)\n",
    "    torch_prediction = best_model.predict_one(input_image).item()\n",
    "    fp_ir_prediction = predict_ir(fp_exec_net, input_image)\n",
    "    int8_ir_prediction = predict_ir(int8_exec_net, input_image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[0][14]\n",
    "    target_label = (\n",
    "        f\"annotation: {data.dataset_train.labels[str(annotation[1].short().item())]}\"\n",
    "    )\n",
    "    torch_predicted_label = f\"torch: {data.dataset_train.labels[str(torch_prediction)]}\"\n",
    "    fp_predicted_label = f\"FP IR: {data.dataset_train.labels[str(fp_ir_prediction)]}\"\n",
    "    int8_predicted_label = (\n",
    "        f\"INT8 IR: {data.dataset_train.labels[str(int8_ir_prediction)]}\"\n",
    "    )\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_title(\n",
    "        f\"{target_label}\\n{torch_predicted_label}\\n{fp_predicted_label}\\n{int8_predicted_label}\"\n",
    "    )\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4983ca-01f1-4867-a297-6f343c65b86c",
   "metadata": {},
   "source": [
    "## Benchmark Performance\n",
    "\n",
    "To measure the inference performance of the FP16 and INT8 models, we use Benchmark Tool, OpenVINO's inference performance measurement tool. Benchmark tool is a command line application that can be run in the notebook with ! benchmark_app or %sx benchmark_app.\n",
    "\n",
    "In the next cell, we create a wrapper function for `benchmark_app` that prints the benchmark_app command with the chosen parameters. For comparison purposes, it filters logging information from the output of `benchmark_app`.\n",
    "\n",
    "> NOTE: For the most accurate performance estimation, we recommended running benchmark_app in a terminal/command prompt after closing other applications. Run `benchmark_app --help` to see all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca87e49-d3d6-4502-8e4b-af64a06da3b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def benchmark_model(\n",
    "    model_path: os.PathLike,\n",
    "    device: str = \"CPU\",\n",
    "    seconds: int = 60,\n",
    "    api: str = \"async\",\n",
    "    batch: int = 1,\n",
    "    cache_dir=\"model_cache\",\n",
    "):\n",
    "    ie = IECore()\n",
    "    model_path = Path(model_path)\n",
    "    if (\"GPU\" in device) and (\"GPU\" not in ie.available_devices):\n",
    "        raise ValueError(\n",
    "            f\"A GPU device is not available. Available devices are: {ie.available_devices}\"\n",
    "        )\n",
    "    else:\n",
    "        benchmark_command = f\"benchmark_app -m {model_path} -d {device} -t {seconds} -api {api} -b {batch} -cdir {cache_dir}\"\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"**Benchmark {model_path.name} with {device} for {seconds} seconds with {api} inference**\"\n",
    "            )\n",
    "        )\n",
    "        display(Markdown(f\"Benchmark command: `{benchmark_command}`\"))\n",
    "\n",
    "        benchmark_output = subprocess.run(\n",
    "            benchmark_command.split(\" \"), capture_output=True, universal_newlines=True\n",
    "        )\n",
    "        benchmark_result = [\n",
    "            line\n",
    "            for line in benchmark_output.stdout.splitlines()\n",
    "            if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")\n",
    "        ]\n",
    "        print(\"\\n\".join(benchmark_result))\n",
    "        print()\n",
    "        if \"MULTI\" in device:\n",
    "            devices = device.replace(\"MULTI:\", \"\").split(\",\")\n",
    "            for single_device in devices:\n",
    "                print(\n",
    "                    f\"{single_device} device: {ie.get_metric(device_name=single_device, metric_name='FULL_DEVICE_NAME')}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                f\"Device: {ie.get_metric(device_name=device, metric_name='FULL_DEVICE_NAME')}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24802c7-6614-4516-83ef-df18a86100dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FP model on CPU\n",
    "benchmark_model(fp_ir_path, device=\"CPU\", seconds=15, api=\"sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafa3cd-9751-42c4-869d-e9560d200a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8 model on CPU\n",
    "benchmark_model(int8_ir_path, device=\"CPU\", seconds=15, api=\"sync\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
